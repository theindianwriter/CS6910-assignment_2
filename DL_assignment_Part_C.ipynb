{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL assignment Part C.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theindianwriter/CS6910-assignment_2/blob/main/DL_assignment_Part_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4EXVKawm3rP"
      },
      "source": [
        "# Video detection with YOLOv3 (For Traffic Analysis)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uGRdi-om6wH"
      },
      "source": [
        "Here the code of ImageAI has been used to detect objects especially traffic analysis. \n",
        "Here is the source code.[ImageAI documentation](https://imageai.readthedocs.io/en/latest/video/index.html). Download the yolo.h5 file and save it in the appropriate location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXvkvSC8eij8"
      },
      "source": [
        "## Install & Load the resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1D2ED1-qmgW",
        "outputId": "597802fd-cfac-46a8-e795-5d9d25d4c54b"
      },
      "source": [
        "!pip install tensorflow==1.13.2\n",
        "!pip install q keras==2.0.6"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.2 in /usr/local/lib/python3.7/dist-packages (1.13.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.32.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.36.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (54.2.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.2) (4.0.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.1)\n",
            "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.6)\n",
            "Requirement already satisfied: keras==2.0.6 in /usr/local/lib/python3.7/dist-packages (2.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.0.6) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras==2.0.6) (1.15.0)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from keras==2.0.6) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from theano->keras==2.0.6) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from theano->keras==2.0.6) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uIMtTTlXfqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea9e39a-918f-4457-8697-46d8c437f345"
      },
      "source": [
        "!pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl\n",
        "!pip install -q opencv-python\n",
        "!pip install -q pillow"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageai==2.0.2 from https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl in /usr/local/lib/python3.7/dist-packages (2.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw_KQVliZi75"
      },
      "source": [
        "from imageai.Detection import VideoObjectDetection\n",
        "import matplotlib as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy\n",
        "import keras\n",
        "import h5py\n",
        "from keras import backend as K"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRHBJBtXe1Yp"
      },
      "source": [
        "## Create an instance of the VideoObjectDetection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz5eb3mAZvpz"
      },
      "source": [
        "detector = VideoObjectDetection()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmCIvOx2fElh"
      },
      "source": [
        "*italicised text*## YOLO v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizGCAsKfHHJ"
      },
      "source": [
        "This function sets the model type of the object detection instance we created to the YOLOv3 model, which means we will be performing our object detection tasks using the pre-trained “YOLOv3” model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUIfQDGzZ0g5"
      },
      "source": [
        "detector.setModelTypeAsYOLOv3()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hHQkwpgfY1H"
      },
      "source": [
        "## Mount the drive to import yolo.h5 and the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kukVv8ubaXWl",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d55854-390c-49e0-d5c9-3c64a4706b68"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa4aeibYbdtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf271f7-65b7-4496-e400-4981b16e205c"
      },
      "source": [
        "!ls '/content/gdrive/My Drive/Yolo/'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yolo.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIZuILNJg36Q"
      },
      "source": [
        "## The model file\n",
        "This function accepts a string which must be the path to the model file we downloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2C88FoldrHP"
      },
      "source": [
        "detector.setModelPath(\"/content/gdrive/My Drive/Yolo/yolo.h5\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR9XmIAHhQpL"
      },
      "source": [
        "## Load the model\n",
        "This function loads the model from the path we specified in the function call above into our object detection instance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkYRCrYEZ6Xd"
      },
      "source": [
        "detector.loadModel()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkwwEt5ZjRGv"
      },
      "source": [
        "## Detect Objects From Video\n",
        "This is the function that performs object detecttion on a video file or video live-feed after the model has been loaded into the instance we created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FYy7y2nqNb8"
      },
      "source": [
        "video_path = detector.detectObjectsFromVideo(input_file_path=\"/content/gdrive/My Drive/Yolo/MUMBAI TRAFFIC _ INDIA.mp4\",\n",
        "                                output_file_path=\"/content/gdrive/My Drive/Yolo/anaylised_video\",\n",
        "                                frames_per_second=29, log_progress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bYWXQdakpph"
      },
      "source": [
        "– *parameter* **input_file_path** (required if you did not set camera_input) : This refers to the path to the video file you want to detect.\n",
        "\n",
        "— *parameter* **output_file_path** (required if you did not set save_detected_video = False) : This refers to the path to which the detected video will be saved. By default, this functionsaves video .avi format.\n",
        "\n",
        "– *parameter* **frames_per_second** (optional , but recommended) : This parameters allows you to set your desired frames per second for the detected video that will be saved. The default value is 20 but we recommend you set the value that suits your video or camera live-feed.\n",
        "\n",
        "— *parameter* **log_progress** (optional) : Setting this parameter to True shows the progress of the video or live-feed as it is detected in the CLI. It will report every frame detected as it progresses. The default value is False.\n",
        "\n",
        "— *parameter* **camera_input** (optional) : This parameter can be set in replacement of the input_file_path if you want to detect objects in the live-feed of a camera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO8LEYSasOAK"
      },
      "source": [
        "#Summary and Conclusion\n",
        "\n",
        "The anaysed video thus obtained can be used for traffic analysis.The number of vehicles can estimated at an instant of time and thus the traffic can be analysed.Also we can share an ip address for the cam on the road to cut live images of the traffic and based on it can to traffic analysis.\n"
      ]
    }
  ]
}