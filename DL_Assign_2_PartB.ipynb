{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assign_2_PartB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theindianwriter/CS6910-assignment_2/blob/main/DL_Assign_2_PartB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6xw8SRBZKgW"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAK4ZOoCUCHU",
        "outputId": "2bc9da24-606b-49c1-d7a5-a866c3ac7fb4"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "!pip install wandb\n",
        "import wandb\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/5d/20ab24504de2669c9a76a50c9bdaeb44a440b0e5e4b92be881ed323857b1/wandb-0.10.26-py2.py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 16.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 50.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 57.5MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.1MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.2.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=00dd9b073f286ce204576f076194b5a4be09a6667d9c9d33a34474562feb1634\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=1e81b3bc5f6e157aafa1e3a91f3086e2393e1ca19076beb9813779c9500a21b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: sentry-sdk, docker-pycreds, smmap, gitdb, GitPython, subprocess32, shortuuid, configparser, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.26\n",
            "PyTorch Version:  1.8.1+cu101\n",
            "Torchvision Version:  0.9.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUaDfnlfZGEP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhV-k3C3ZP7h"
      },
      "source": [
        "#Checking for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjFzYN6rUcQU",
        "outputId": "da694b4b-73ac-4deb-db97-e38bf616e490"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U7JOXg9WP8C"
      },
      "source": [
        "#Downloading the dataset and removing .DStore "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xMCdcfuZYm5"
      },
      "source": [
        "#we used curlwget to download the data for part B"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5WuVd7hUdpa",
        "outputId": "1ae3d66b-45ed-480c-fc22-eedbe9015d30"
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36 Edg/89.0.774.68\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\" -c -O 'nature_12K.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-16 11:20:42--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.73.240, 172.217.15.112, 172.217.164.144, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.73.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3816687935 (3.6G) [application/zip]\n",
            "Saving to: ‘nature_12K.zip’\n",
            "\n",
            "nature_12K.zip       60%[===========>        ]   2.14G  31.5MB/s    eta 17s    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuzRvH8mUhAL"
      },
      "source": [
        "!unzip \"/content/nature_12K.zip\"\n",
        "print(\"folders unzipped\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onxIeuveWtuQ"
      },
      "source": [
        "cd \"/content/inaturalist_12K/train\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrmjEK4mW4Jd"
      },
      "source": [
        "!rm -rf \".DS_Store\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUBoinvTW8Kh"
      },
      "source": [
        "cd \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOP8HHXKRCHT"
      },
      "source": [
        "##Dividing Data into train test and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1uwAhZtUiIz"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "training_folder_name = '/content/inaturalist_12K/train'\n",
        "\n",
        "# New location for \n",
        "validation_folder = '/content/datasets/validation'\n",
        "\n",
        "# Create the output folder if it doesn't already exist\n",
        "if os.path.exists(validation_folder):\n",
        "    shutil.rmtree(validation_folder)\n",
        "\n",
        "# Loop through each subfolder in the input folder\n",
        "print('Moving images...')\n",
        "for root, folders, files in os.walk(training_folder_name):\n",
        "    for sub_folder in folders:\n",
        "        if sub_folder == \".DS_Store\":\n",
        "            continue\n",
        "        print('processing folder ' + sub_folder)\n",
        "        # Create a matching subfolder in the output dir\n",
        "        saveFolder = os.path.join(validation_folder,sub_folder)\n",
        "        if not os.path.exists(saveFolder):\n",
        "            os.makedirs(saveFolder)\n",
        "        # Loop through the files in the subfolder\n",
        "        file_names = os.listdir(os.path.join(root,sub_folder))\n",
        "        import random\n",
        "        index=random.sample(range(0, len(file_names)), int(0.1*len(file_names)))\n",
        "        for i in index:\n",
        "            file_name = file_names[i]\n",
        "\n",
        "            if file_name == \".DS_Store\":\n",
        "                continue\n",
        "            file_path = os.path.join(root,sub_folder, file_name)\n",
        "    \n",
        "            shutil.move(file_path , saveFolder)\n",
        "\n",
        "print('Done.')\n",
        "print('renaming val folder as test folder')\n",
        "#os.rename('/content/inaturalist_12K/val','/content/inaturalist_12K/test')\n",
        "shutil.move('/content/inaturalist_12K/val' ,'/content/datasets/test')\n",
        "shutil.move('/content/inaturalist_12K/train' ,'/content/datasets/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWBE0uMQtWM_"
      },
      "source": [
        "#**trainModel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWaL_-G9yYp1"
      },
      "source": [
        "def trainModel(model, train_loader, criterion, optimizer,scheduler, epoch, is_inception=False):\n",
        "    scheduler.step()\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    best_acc = 0.0\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            if is_inception:\n",
        "                outputs, aux_outputs = model(inputs)\n",
        "                loss1 = criterion(outputs, labels)\n",
        "                loss2 = criterion(aux_outputs, labels)\n",
        "                loss = loss1 + 0.3*loss2\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #del inputs, labels, outputs\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        #statistics\n",
        "        since = time.time()\n",
        "       \n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    #wandb.log({\"Train Loss\": epoch_loss})\n",
        "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "    #wandb.log({\"Train accuracy\": epoch_acc})\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('train', epoch_loss, epoch_acc))\n",
        "\n",
        "    avg_loss = train_loss/(batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    #return model, avg_loss\n",
        "    #return model,avg_loss\n",
        "    return avg_loss,model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J3jZl2d7F4l"
      },
      "source": [
        "#Test Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUfoisgN7NSd"
      },
      "source": [
        "def testModel(model, device, test_loader,criterion):\n",
        "    since = time.time()\n",
        "    val_acc_history = []\n",
        "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    # Switch the model to evaluation mode ]\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            batch_count += 1\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            # Calculate the loss for this batch\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += criterion(outputs, labels).item()\n",
        "            \n",
        "            # Calculate the accuracy for this batch\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(labels==preds).item()\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    #wandb.log({\"Val loss\": epoch_loss})\n",
        "    epoch_acc = running_corrects.double() / len(test_loader.dataset)\n",
        "    avg_loss = test_loss / batch_count\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('val', epoch_loss, epoch_acc))\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, running_corrects, len(test_loader.dataset),\n",
        "        100. * running_corrects / len(test_loader.dataset)))\n",
        "\n",
        "    #wandb.log({\"Val accuracy\": epoch_acc})\n",
        "\n",
        "\n",
        "    return avg_loss,epoch_acc,model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pe9eIAsJh4d"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AbadhiGicS"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "  \n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        print(\"resnet called\")\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet34\":\n",
        "        \"\"\" Resnet 34\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet34(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet50\":\n",
        "        \"\"\" Resnet 50\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet101\":\n",
        "        \"\"\" Resnet101\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet101(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg11\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "    elif model_name == \"vgg19\":\n",
        "        \"\"\" VGG19_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet121\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "    elif model_name == \"inceptionv3\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "        \n",
        "    elif model_name == \"densenet161\":\n",
        "        \"\"\" densenet161\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet161(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "    \n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIx1IiSfn4is"
      },
      "source": [
        "#Dataset Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zinn6yCrZ4Bu"
      },
      "source": [
        "def Dataset_Transform(input_size=400,train_data_directory='/content/datasets/train',val_data_directory = '/content/datasets/validation',\n",
        "                      test_data_directory = '/content/datasets/test',batch_size = 8):\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(input_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        transform_val = transforms.Compose([\n",
        "            transforms.Resize(input_size),\n",
        "            transforms.CenterCrop(input_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.Resize(input_size),\n",
        "            transforms.CenterCrop(input_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        train_set = torchvision.datasets.ImageFolder(\n",
        "            root=train_data_directory,\n",
        "            transform=transform_train\n",
        "        )\n",
        "        val_set = torchvision.datasets.ImageFolder(\n",
        "            root=val_data_directory,\n",
        "            transform=transform_val\n",
        "        )\n",
        "        test_set = torchvision.datasets.ImageFolder(\n",
        "            root=test_data_directory,\n",
        "            transform=transform_test\n",
        "        )\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_set,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=2,\n",
        "            shuffle=True\n",
        "        )\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            val_set,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=2,\n",
        "            shuffle=False\n",
        "        )\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            test_set,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=2,\n",
        "            shuffle=False\n",
        "        )\n",
        "        dataloaders_dict = {}\n",
        "        dataloaders_dict['train'] = train_loader\n",
        "        dataloaders_dict['val'] = val_loader\n",
        "        dataloaders_dict['test'] = test_loader\n",
        "\n",
        "        return dataloaders_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOYkEYhhTr2c"
      },
      "source": [
        "## Function for selectively tuning layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGA2b63oTdBi"
      },
      "source": [
        " def unfreeze_layers(x,model_ft):\n",
        "        layers =[]\n",
        "\n",
        "        for name, child in model_ft.named_children():\n",
        "            layers.append(name)\n",
        "        index = range(-1,-1*min(x+1,len(layers)),-1)\n",
        "        layers_to_unfreeze = []\n",
        "        for id in index:\n",
        "            layers_to_unfreeze.append(layers[id])\n",
        "        for param in model_ft.parameters():\n",
        "            param.requires_grad = True\n",
        "        for name, child in model_ft.named_children():\n",
        "            if name in  layers_to_unfreeze:\n",
        "                print(name + ' is unfrozen')\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "            else:\n",
        "                print(name + ' is frozen')\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = False\n",
        "        return model_ft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ4I1JqxT9i-"
      },
      "source": [
        "#Fitting Model to device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYovOewnUBBj"
      },
      "source": [
        "def fit_model_to_device(model_ft,device,feature_extract):\n",
        "    # Send the model to GPU\n",
        "    model_ft = model_ft.to(device)\n",
        "    params_to_update = model_ft.parameters()\n",
        "    print(\"Params to learn:\")\n",
        "    if feature_extract:\n",
        "        params_to_update = []\n",
        "        for name,param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                params_to_update.append(param)\n",
        "                print(\"\\t\",name)\n",
        "    else:\n",
        "        for name,param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                print(\"\\t\",name)\n",
        "    return model_ft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rsQ-vxoXHbk"
      },
      "source": [
        "#Fitting Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHpp7rnYXJvN"
      },
      "source": [
        "def fittinng_model(model_name,model_ft,optimizer,scheduler,criterion,epochs,dataloaders_dict):\n",
        "    test_loader = dataloaders_dict['test']\n",
        "    train_loader = dataloaders_dict['train']\n",
        "    val_loader= dataloaders_dict['val']\n",
        "    #print(criterion)\n",
        "    # Track metrics in these arrays\n",
        "    epoch_nums = []\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model_ft.state_dict())\n",
        "    print('Training on', device)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "            #torch.cuda.empty_cache()\n",
        "            train_loss,model_ft =  trainModel(model_ft, train_loader, criterion, optimizer, scheduler,epoch,is_inception=(model_name == \"inceptionv3\"))\n",
        "            #train_loss,model_ft= trainModel(model_ft, device, train_loader, optimizer, epoch)\n",
        "            val_loss,epoch_acc,model_ft = testModel(model_ft, device,val_loader,criterion)\n",
        "            if(epoch_acc > best_acc):\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model_ft.state_dict())\n",
        "            \n",
        "            print('Best val Acc: {:4f}'.format(best_acc))\n",
        "            epoch_nums.append(epoch)\n",
        "            training_loss.append(train_loss)\n",
        "            validation_loss.append(val_loss)\n",
        "    # load best model weights\n",
        "    model_ft.load_state_dict(best_model_wts)\n",
        "    return  validation_loss, model_ft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRl7UQKK54lq"
      },
      "source": [
        "#Function to evaluate train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy76QVJg1kfw"
      },
      "source": [
        "def evaluateModel(dataloader, model,is_inception):\n",
        "    total, correct = 0, 0\n",
        "    for data in dataloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs= model(inputs)\n",
        "        _, pred = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "    return 100 * correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdEMBeIITyOk"
      },
      "source": [
        "#Initializing the model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBCTFupgyYAn"
      },
      "source": [
        "#model_name =\"inception\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUNru4bM5nIN"
      },
      "source": [
        "train_data_directory='/content/datasets/train'\n",
        "val_data_directory = '/content/datasets/validation'\n",
        "test_data_directory = '/content/datasets/test',\n",
        "def startTraining(model_name, num_classess, batch_size, num_epochs,feature_extract,no_layers_to_unfreeze,lr,gamma,opt):\n",
        "\n",
        "    model_ft, input_size = initialize_model(model_name=model_name, num_classes=10, feature_extract= feature_extract, use_pretrained=True)\n",
        "\n",
        "    model_ft = model_ft.to(device)\n",
        "    # Print the model we just instantiated\n",
        "    #print(model_ft)\n",
        "\n",
        "    dataloaders_dict=Dataset_Transform(input_size=input_size,train_data_directory='/content/datasets/train',val_data_directory = '/content/datasets/validation',\n",
        "                        test_data_directory = '/content/datasets/test',batch_size = batch_size)\n",
        "    model_ft = unfreeze_layers(no_layers_to_unfreeze,model_ft)\n",
        "    model_ft = fit_model_to_device(model_ft,device,True)\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft  = None\n",
        "    if(opt == \"sgd\"):\n",
        "        # Decay LR by a factor of 0.1 every 7 epochs\n",
        "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=0.9)\n",
        "    elif(opt == \"adam\"):\n",
        "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.000001, amsgrad=False)\n",
        "    elif(opt==\"rmsp\"):\n",
        "        optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=lr, alpha=0.99, eps=1e-08, weight_decay=0.0001, momentum=0.9, centered=False)\n",
        "    elif(opt==\"adagrad\"):\n",
        "        optimizer_ft = optim.Adagrad(model_ft.parameters(), lr=lr, lr_decay=0, weight_decay=0.0001, initial_accumulator_value=0, eps=1e-10)\n",
        "    else:\n",
        "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=gamma)\n",
        "\n",
        "    # Specify the loss criteria\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    validation_loss, model_ft = fittinng_model(model_name = model_name,model_ft=model_ft,optimizer=optimizer_ft,scheduler=exp_lr_scheduler,criterion=criterion,epochs=num_epochs,dataloaders_dict=dataloaders_dict)\n",
        "    #Testing Model on Test Data Set\n",
        "    Test_accuracy = evaluateModel(dataloaders_dict['test'], model_ft,is_inception=(model_name == \"inceptionv3\"))\n",
        "    print('Test Accuracy: {:4f}'.format(Test_accuracy))\n",
        "    return model_ft\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndRuB4hUh3zI"
      },
      "source": [
        "#uncomment below part and comment wandb logs in testModel and trainModel function to run without wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idSDpUJGxEhc"
      },
      "source": [
        "startTraining(model_name =\"densenet161\", num_classess =10, batch_size =32, num_epochs=2,feature_extract=False,no_layers_to_unfreeze=1,lr=0.001,gamma=0.1,opt = \"sgd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL8VWq2f2R7g"
      },
      "source": [
        "#Wandb Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnbaOHQy2sAJ"
      },
      "source": [
        "sweep_config = {\n",
        "    'name': 'Hyper_parameter_check',\n",
        "    'method': 'random', #grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters':{\n",
        "        'model_name':{\n",
        "           'values' :[\"inceptionv3\",\"resnet50\",\"alexnet\", \"vgg11\", \"squeezenet\", \"densenet161\",\"resnet101\"]\n",
        "        },\n",
        "        'max_epoch':{\n",
        "            'values':[2,5,10]\n",
        "        },\n",
        "        'batch_size':{\n",
        "           'values' :[32,64,128]\n",
        "        },\n",
        "        'layers_to_train':{\n",
        "            'values':[1,2,3,4]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            'values':[1e-2,1e-3,1e-4]\n",
        "        },\n",
        "        'gamma':{\n",
        "            'values': [0.1,0.2,0.3]\n",
        "        },\n",
        "        'optimizer':{\n",
        "            'values': [\"adam\",\"adagrad\",\"sgd\",\"rmsp\"]\n",
        "        }\n",
        "\n",
        "    },\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh9vSQiM3rQa"
      },
      "source": [
        "wandb.login()\n",
        "wandb.init(project = \"DL_Assign_2B\", entity = \"cs20m038\")\n",
        "sweep_id = wandb.sweep(sweep_config, project='DL_Assign_2B', entity='cs20m038')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRwrFs6541uy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXUNcQLzK0GC"
      },
      "source": [
        "def runSweep():\n",
        "    run = wandb.init()\n",
        "    configuration = run.config\n",
        "    startTraining(model_name = configuration.model_name, num_classess = 10, batch_size = configuration.batch_size, num_epochs=configuration.max_epoch,feature_extract = False,no_layers_to_unfreeze=configuration.layers_to_train,lr=configuration.learning_rate,gamma=configuration.gamma,opt = configuration.optimizer)\n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trkaH6Dw92rt"
      },
      "source": [
        "wandb.agent(sweep_id=sweep_id, function=runSweep)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}